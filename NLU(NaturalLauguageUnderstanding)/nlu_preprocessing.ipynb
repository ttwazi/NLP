{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948cfbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import git\n",
    "#git.Git(\"../\").clone(\"https://github.com/sonos/nlu-benchmark\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e092f740",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 11:38:08.373220: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d2df8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './nlu-benchmark/2017-06-custom-intent-engines/'\n",
    "DEFAULT_SLOT_LABEL = 'unspecified'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d014a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_labels = [ domain for domain in os.listdir(DATA_PATH) if os.path.isdir(DATA_PATH  + domain) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9638213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_train_x = []\n",
    "cls_train_y = []\n",
    "cls_val_x = []\n",
    "cls_val_y = []\n",
    "\n",
    "Weather_seq_l_train_x = []\n",
    "Weather_seq_l_train_y = []\n",
    "Weather_seq_l_val_x = []\n",
    "Weather_seq_l_val_y = []\n",
    "weather_seq_label_set = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2e3a7ca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data( domain, dataset_type, full):\n",
    "    file_path = os.path.join(DATA_PATH, \"{}/{}_{}{}.json\".format(domain, dataset_type, domain, full))\n",
    "    with open(file_path, 'r') as filename:\n",
    "        print(filename)\n",
    "        dataset = json.load(filename)[domain]\n",
    "    \n",
    "    cls_x = []\n",
    "    cls_y = []\n",
    "    seq_l_x = []\n",
    "    seq_l_y = []\n",
    "    for element in dataset:\n",
    "        tokenized_sentence = []\n",
    "        seq_labels = []\n",
    "        for piece in element['data']:\n",
    "            words = piece['text'].strip().split()\n",
    "            tokenized_sentence += words\n",
    "            for token in words:\n",
    "                if 'entity' in piece.keys():\n",
    "                    seq_labels += [piece['entity']]\n",
    "                else:\n",
    "                    seq_labels.append(DEFAULT_SLOT_LABEL)\n",
    "        cls_x.append(tokenized_sentence)\n",
    "        cls_y.append(domain)\n",
    "        seq_l_x.append(tokenized_sentence)\n",
    "        seq_l_y.append(seq_labels)\n",
    "    return cls_x, cls_y, seq_l_x, seq_l_y\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bdc28c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/RateBook/train_RateBook_full.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/RateBook/validate_RateBook.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/SearchCreativeWork/train_SearchCreativeWork_full.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/SearchCreativeWork/validate_SearchCreativeWork.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/GetWeather/train_GetWeather_full.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/GetWeather/validate_GetWeather.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/BookRestaurant/train_BookRestaurant_full.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/BookRestaurant/validate_BookRestaurant.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/AddToPlaylist/train_AddToPlaylist_full.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/AddToPlaylist/validate_AddToPlaylist.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/SearchScreeningEvent/train_SearchScreeningEvent_full.json' mode='r' encoding='UTF-8'>\n",
      "<_io.TextIOWrapper name='./nlu-benchmark/2017-06-custom-intent-engines/SearchScreeningEvent/validate_SearchScreeningEvent.json' mode='r' encoding='UTF-8'>\n"
     ]
    }
   ],
   "source": [
    "for domain in cls_labels:\n",
    "    cls_x, cls_y, seq_l_train_x, seq_l_train_y = load_data(domain, 'train', '_full')\n",
    "    cls_train_x += cls_x \n",
    "    cls_train_y += cls_y\n",
    "    \n",
    "    cls_x, cls_y, seq_l_val_x, seq_l_val_y = load_data(domain, 'validate', '')\n",
    "    cls_val_x += cls_x\n",
    "    cls_val_y += (cls_y)\n",
    "    \n",
    "    if domain == 'GetWeather':\n",
    "        Weather_seq_l_train_x = seq_l_train_x\n",
    "        Weather_seq_l_train_y = seq_l_train_y\n",
    "        Weather_seq_l_val_x = seq_l_val_x\n",
    "        Weather_seq_l_val_y = seq_l_val_y\n",
    "        for seq_label in Weather_seq_l_train_y:\n",
    "            weather_seq_label_set |= set(seq_label)\n",
    "        for seq_label in Weather_seq_l_val_y:\n",
    "            weather_seq_label_set |= set(seq_label)            \n",
    "    \n",
    "weather_seq_label_list = list(weather_seq_label_set)\n",
    "weather_seq_label_list.remove('unspecified')\n",
    "weather_seq_label_list.insert(0, 'unspecified')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12f808b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(train_x, train_y, valid_x, valid_y, labels, task = 'classification', max_len=64):\n",
    "\n",
    "    special_tokens = ['<pad>', '<unk>']\n",
    "    word2ind = {}\n",
    "    ind2word = {}\n",
    "    \n",
    "    def add_word(word2ind, ind2word, word):\n",
    "        if word in word2ind.keys():\n",
    "            return\n",
    "        word2ind[word] = len(ind2word)\n",
    "        ind2word[len(ind2word)] = word\n",
    "    for token in special_tokens:\n",
    "        add_word(word2ind, ind2word, token)\n",
    "        \n",
    "    for sent in train_x:\n",
    "        for word in sent:\n",
    "            add_word(word2ind, ind2word, word)\n",
    "\n",
    "    train_x_ids = []\n",
    "    for sent in train_x:\n",
    "        sent_ids = []\n",
    "        for word in sent:\n",
    "            sent_ids.append(word2ind.get(word, word2ind['<unk>']))\n",
    "        train_x_ids.append(sent_ids)\n",
    "        \n",
    "    train_x_ids = keras.preprocessing.sequence.pad_sequences(train_x_ids, maxlen = max_len, padding = 'post', value = word2ind['<pad>'])\n",
    "     \n",
    "    val_x_ids = []\n",
    "    for sent in valid_x:\n",
    "        sent_ids = []\n",
    "        for word in sent:\n",
    "            sent_ids.append(word2ind.get(word, word2ind['<unk>']))\n",
    "        val_x_ids.append(sent_ids)\n",
    "        \n",
    "    val_x_ids = keras.preprocessing.sequence.pad_sequences(val_x_ids, maxlen = max_len, padding = 'post', value = word2ind['<pad>'])\n",
    "       \n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    if task == 'classification':\n",
    "        train_y_ids = [labels.index(label) for label in train_y]\n",
    "        val_y_ids = [labels.index(label) for label in valid_y]\n",
    "    #classification data vectorization\n",
    "\n",
    "    elif task == 'slot_labelling':\n",
    "        train_y_ids =  [[labels.index(label) for label in sent] for sent in train_y]\n",
    "        train_y_ids = keras.preprocessing.sequence.pad_sequences(train_y_ids, maxlen = max_len, padding = 'post', value = word2ind['<pad>'])\n",
    "        val_y_ids =  [[labels.index(label) for label in sent] for sent in valid_y]\n",
    "        val_y_ids = keras.preprocessing.sequence.pad_sequences(val_y_ids, maxlen = max_len, padding = 'post', value = word2ind['<pad>'])\n",
    "          \n",
    "\n",
    "    return word2ind, train_x_ids, train_y_ids, val_x_ids,  val_y_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b5d07fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_word2ind, cls_train_x_ids, cls_train_y_ids, cls_val_x_ids, cls_val_y_ids = vectorization(cls_train_x, cls_train_y,cls_val_x, \\\n",
    "                                                                                             cls_val_y, cls_labels, 'classification' )\n",
    "Weather_seq_word2ind, Weather_seq_l_train_x_ids, Weather_seq_l_train_y_ids, \\\n",
    "    Weather_seq_l_val_x_ids, Weather_seq_l_val_y_ids =  vectorization(Weather_seq_l_train_x, Weather_seq_l_train_y,\\\n",
    "                                                                      Weather_seq_l_val_x, Weather_seq_l_val_y, weather_seq_label_list, 'slot_labelling')\n",
    "\n",
    "Weather_seq_sample_weights = np.ones(Weather_seq_l_train_y_ids.shape)\n",
    "for i, sequence in enumerate(Weather_seq_l_train_y_ids):\n",
    "    for j, label in enumerate(sequence):\n",
    "        if label == weather_seq_label_list.index('unspecified'):\n",
    "            Weather_seq_sample_weights[i][j] = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "211bc875",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cls_data = {'word2ind': cls_word2ind,\n",
    "           'label_list': cls_labels,\n",
    "           'train_x': cls_train_x_ids,\n",
    "           'train_y': cls_train_y_ids,\n",
    "           'val_x': cls_val_x_ids,\n",
    "           'val_y': cls_val_y_ids}\n",
    "\n",
    "weather_seq_data = {'word2ind': Weather_seq_word2ind,\n",
    "           'label_list': weather_seq_label_list,\n",
    "           'sample_weight': Weather_seq_sample_weights,\n",
    "           'train_x': Weather_seq_l_train_x_ids,\n",
    "           'train_y': Weather_seq_l_train_y_ids,\n",
    "           'val_x': Weather_seq_l_val_x_ids,\n",
    "           'val_y': Weather_seq_l_val_y_ids}\n",
    "pickle.dump(cls_data, open('cls_data.pickle', 'wb'))\n",
    "pickle.dump(weather_seq_data, open('weather_seq_data.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
